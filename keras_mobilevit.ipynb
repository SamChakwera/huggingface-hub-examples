{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_mobilevit",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2fa20d982beb464d9e776ac5faf58002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_78fa9b127bcf4f07a92e5f0724dd9b7c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2dc5fb520f064b8e8ab75e293af6cf15",
              "IPY_MODEL_af34bc8d60cc4964a366eca342e890fc",
              "IPY_MODEL_094b25b750934cbda7318a42aa28d378"
            ]
          }
        },
        "78fa9b127bcf4f07a92e5f0724dd9b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2dc5fb520f064b8e8ab75e293af6cf15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9f2a06b8c7fc42239ea2e9e813334959",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Upload file variables/variables.data-00000-of-00001: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c39ae1be7d343ac87e582f86cad0426"
          }
        },
        "af34bc8d60cc4964a366eca342e890fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58b5302d0d394793adb1777883caf19a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 15859640,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15859640,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad45153867e84d0ca95b068795e2de50"
          }
        },
        "094b25b750934cbda7318a42aa28d378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8a46cf77f32d4271b6e1aad127f325f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15.1M/15.1M [00:14&lt;00:00, 1.58MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45f2cdd52d7547bc9b386c1ef1c6e130"
          }
        },
        "9f2a06b8c7fc42239ea2e9e813334959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c39ae1be7d343ac87e582f86cad0426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58b5302d0d394793adb1777883caf19a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad45153867e84d0ca95b068795e2de50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a46cf77f32d4271b6e1aad127f325f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45f2cdd52d7547bc9b386c1ef1c6e130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e986d9c150884880b5642ac4d4a720c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b8f9791b270947ae81b06593c5488333",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_143f919ec7a1441989b3bde75d7eb503",
              "IPY_MODEL_67a835314e864046972f4831a3aadf09",
              "IPY_MODEL_dfcef6d7579842d0a4615cd0658fab77"
            ]
          }
        },
        "b8f9791b270947ae81b06593c5488333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "143f919ec7a1441989b3bde75d7eb503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83171e6892c8484daeb1d273317223fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Upload file keras_metadata.pb: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_328c94a6e9ef47a0abce08a5ab8b9445"
          }
        },
        "67a835314e864046972f4831a3aadf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_959b4d991d0f4e6d848ac67afd6839f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 366476,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 366476,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5a97a2c046e4449b99cdfb226d209d8"
          }
        },
        "dfcef6d7579842d0a4615cd0658fab77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fd5a8fceb9424d4a867294478a49de3d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 358k/358k [00:14&lt;00:00, 25.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_64f07caa84e3478c85c6c8c12f371e0d"
          }
        },
        "83171e6892c8484daeb1d273317223fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "328c94a6e9ef47a0abce08a5ab8b9445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "959b4d991d0f4e6d848ac67afd6839f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5a97a2c046e4449b99cdfb226d209d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd5a8fceb9424d4a867294478a49de3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "64f07caa84e3478c85c6c8c12f371e0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d36f6585a50745818991e9bb21fc9dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3abeebd840f64aa88b24790cad9e2c99",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9f5165422c8f46528987200122fc34b0",
              "IPY_MODEL_0c6ce09eaf344a22b89d369ecdc454fa",
              "IPY_MODEL_61718b42d97e4b7fbf928bc0b3cb548a"
            ]
          }
        },
        "3abeebd840f64aa88b24790cad9e2c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f5165422c8f46528987200122fc34b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ca3478aea394bc6b96f0632ff52d82a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Upload file saved_model.pb: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a66f54459f4d4006ab5db0625ebca351"
          }
        },
        "0c6ce09eaf344a22b89d369ecdc454fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_426bab5dd71d453f8e1a34ffc1680ec7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5006492,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5006492,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df8121f43bff42858dfc0f79769f515c"
          }
        },
        "61718b42d97e4b7fbf928bc0b3cb548a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3650b1615abb4864a265455723e16d4b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.77M/4.77M [00:14&lt;00:00, 200kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb9d04dfcd784f0d8c0c7904d4dd8d3e"
          }
        },
        "0ca3478aea394bc6b96f0632ff52d82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a66f54459f4d4006ab5db0625ebca351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "426bab5dd71d453f8e1a34ffc1680ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df8121f43bff42858dfc0f79769f515c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3650b1615abb4864a265455723e16d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb9d04dfcd784f0d8c0c7904d4dd8d3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nateraw/huggingface-hub-examples/blob/main/keras_mobilevit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp_bZKHCL01y"
      },
      "source": [
        "# MobileViT: A mobile-friendly Transformer-based model for image classification\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/10/20<br>\n",
        "**Last modified:** 2021/10/20<br>\n",
        "**Description:** MobileViT for image classification with combined benefits of convolutions and Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwj7XT_XL010"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we implement the MobileViT architecture\n",
        "([Mehta et al.](https://arxiv.org/abs/2110.02178)),\n",
        "which combines the benefits of Transformers\n",
        "([Vaswani et al.](https://arxiv.org/abs/1706.03762))\n",
        "and convolutions. With Transformers, we can capture long-range dependencies that result\n",
        "in global representations. With convolutions, we can capture spatial relationships that\n",
        "model locality.\n",
        "\n",
        "Besides combining the properties of Transformers and convolutions, the authors introduce\n",
        "MobileViT as a general-purpose mobile-friendly backbone for different image recognition\n",
        "tasks. Their findings suggest that, performance-wise, MobileViT is better than other\n",
        "models with the same or higher complexity ([MobileNetV3](https://arxiv.org/abs/1905.02244),\n",
        "for example), while being efficient on mobile devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB27tmLBL011"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3iNvqXPMB-A",
        "outputId": "e50d8c5c-cc9d-4755-d636-785e32469ccc"
      },
      "source": [
        "! pip install tensorflow-addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APUoxck5L011"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.applications import imagenet_utils\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_3Kh_8NL012"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU8lowy2L012"
      },
      "source": [
        "# Values are from table 4.\n",
        "patch_size = 4  # 2x2, for the Transformer blocks.\n",
        "image_size = 256\n",
        "expansion_factor = 2  # expansion factor for the MobileNetV2 blocks."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU6maqn0L013"
      },
      "source": [
        "## MobileViT utilities\n",
        "\n",
        "The MobileViT architecture is comprised of the following blocks:\n",
        "\n",
        "* Strided 3x3 convolutions that process the input image.\n",
        "* [MobileNetV2](https://arxiv.org/abs/1801.04381)-style inverted residual blocks for\n",
        "downsampling the resolution of the intermediate feature maps.\n",
        "* MobileViT blocks that combine the benefits of Transformers and convolutions. It is\n",
        "presented in the figure below (taken from the\n",
        "[original paper](https://arxiv.org/abs/2110.02178)):\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/mANnhI7.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-ZRH-pmL014"
      },
      "source": [
        "\n",
        "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
        "    conv_layer = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, activation=tf.nn.swish, padding=\"same\"\n",
        "    )\n",
        "    return conv_layer(x)\n",
        "\n",
        "\n",
        "# Reference: https://git.io/JKgtC\n",
        "\n",
        "\n",
        "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
        "    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    if strides == 2:\n",
        "        m = layers.ZeroPadding2D(padding=imagenet_utils.correct_pad(m, 3))(m)\n",
        "    m = layers.DepthwiseConv2D(\n",
        "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
        "    )(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "\n",
        "    if tf.math.equal(x.shape[-1], output_channels) and strides == 1:\n",
        "        return layers.Add()([m, x])\n",
        "    return m\n",
        "\n",
        "\n",
        "# Reference:\n",
        "# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
        "\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.swish)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=[x.shape[-1] * 2, x.shape[-1]], dropout_rate=0.1,)\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
        "    local_features = conv_block(\n",
        "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
        "    )\n",
        "\n",
        "    # Unfold into patches and then pass through Transformers.\n",
        "    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n",
        "    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n",
        "        local_features\n",
        "    )\n",
        "    global_features = transformer_block(\n",
        "        non_overlapping_patches, num_blocks, projection_dim\n",
        "    )\n",
        "\n",
        "    # Fold into conv-like feature-maps.\n",
        "    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
        "        global_features\n",
        "    )\n",
        "\n",
        "    # Apply point-wise conv -> concatenate with the input features.\n",
        "    folded_feature_map = conv_block(\n",
        "        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
        "\n",
        "    # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features = conv_block(\n",
        "        local_global_features, filters=projection_dim, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKrTsF9BL015"
      },
      "source": [
        "**More on the MobileViT block**:\n",
        "\n",
        "* First, the feature representations (A) go through convolution blocks that capture local\n",
        "relationships. The expected shape of a single entry here would be `(h, w, num_channels)`.\n",
        "* Then they get unfolded into another vector with shape `(p, n, num_channels)`,\n",
        "where `p` is the area of a small patch, and `n` is `(h * w) / p`. So, we end up with `n`\n",
        "non-overlapping patches.\n",
        "* This unfolded vector is then passed through a Tranformer block that captures global\n",
        "relationships between the patches.\n",
        "* The output vector (B) is again folded into a vector of shape `(h, w, num_channels)`\n",
        "resembling a feature map coming out of convolutions.\n",
        "\n",
        "Vectors A and B are then passed through two more convolutional layers to fuse the local\n",
        "and global representations. Notice how the spatial resolution of the final vector remains\n",
        "unchanged at this point. The authors also present an explanation of how the MobileViT\n",
        "block resembles a convolution block of a CNN. For more details, please refer to the\n",
        "original paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbCLH9fEL016"
      },
      "source": [
        "Next, we combine these blocks together and implement the MobileViT architecture (XXS\n",
        "variant). The following figure (taken from the original paper) presents a schematic\n",
        "representation of the architecture:\n",
        "\n",
        "![](https://i.ibb.co/sRbVRBN/image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgcdO2HsL016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a9e65a-5358-4511-c6ea-33c783f4da8c"
      },
      "source": [
        "\n",
        "def create_mobilevit(num_classes=5):\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    x = conv_block(x, filters=16)\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=16\n",
        "    )\n",
        "\n",
        "    # Downsampling with MV2 block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "\n",
        "    # First MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n",
        "    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n",
        "\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "mobilevit_xxs = create_mobilevit()\n",
        "mobilevit_xxs.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescaling (Rescaling)           (None, 256, 256, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 128, 128, 16) 448         rescaling[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 32) 512         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu (TFOpLambda)         (None, 128, 128, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseConv (None, 128, 128, 32) 288         tf.nn.silu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         depthwise_conv2d[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_1 (TFOpLambda)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 16) 512         tf.nn.silu_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128, 128, 16) 64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 128, 128, 16) 0           batch_normalization_2[0][0]      \n",
            "                                                                 conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 128, 128, 32) 512         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 128, 32) 128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_2 (TFOpLambda)       (None, 128, 128, 32) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 129, 129, 32) 0           tf.nn.silu_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "depthwise_conv2d_1 (DepthwiseCo (None, 64, 64, 32)   288         zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 32)   128         depthwise_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_3 (TFOpLambda)       (None, 64, 64, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 24)   768         tf.nn.silu_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 64, 24)   96          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 48)   1152        batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 64, 48)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_4 (TFOpLambda)       (None, 64, 64, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "depthwise_conv2d_2 (DepthwiseCo (None, 64, 64, 48)   432         tf.nn.silu_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 64, 64, 48)   192         depthwise_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_5 (TFOpLambda)       (None, 64, 64, 48)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 64, 64, 24)   1152        tf.nn.silu_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 64, 64, 24)   96          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 64, 64, 24)   0           batch_normalization_8[0][0]      \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 64, 64, 48)   1152        add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 64, 64, 48)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_6 (TFOpLambda)       (None, 64, 64, 48)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "depthwise_conv2d_3 (DepthwiseCo (None, 64, 64, 48)   432         tf.nn.silu_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 64, 64, 48)   192         depthwise_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_7 (TFOpLambda)       (None, 64, 64, 48)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 64, 64, 24)   1152        tf.nn.silu_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 64, 64, 24)   96          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 64, 64, 24)   0           batch_normalization_11[0][0]     \n",
            "                                                                 add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 64, 64, 48)   1152        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 64, 64, 48)   192         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_8 (TFOpLambda)       (None, 64, 64, 48)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 65, 65, 48)   0           tf.nn.silu_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "depthwise_conv2d_4 (DepthwiseCo (None, 32, 32, 48)   432         zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 48)   192         depthwise_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_9 (TFOpLambda)       (None, 32, 32, 48)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 48)   2304        tf.nn.silu_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 48)   192         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 64)   27712       batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 64)   4160        conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 4, 256, 64)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization (LayerNorma (None, 4, 256, 64)   128         reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention (MultiHead (None, 4, 256, 64)   33216       layer_normalization[0][0]        \n",
            "                                                                 layer_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 4, 256, 64)   0           multi_head_attention[0][0]       \n",
            "                                                                 reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_1 (LayerNor (None, 4, 256, 64)   128         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 4, 256, 128)  8320        layer_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4, 256, 128)  0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4, 256, 64)   8256        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 4, 256, 64)   0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 4, 256, 64)   0           dropout_1[0][0]                  \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_2 (LayerNor (None, 4, 256, 64)   128         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_1 (MultiHe (None, 4, 256, 64)   33216       layer_normalization_2[0][0]      \n",
            "                                                                 layer_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 4, 256, 64)   0           multi_head_attention_1[0][0]     \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_3 (LayerNor (None, 4, 256, 64)   128         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 4, 256, 128)  8320        layer_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 4, 256, 128)  0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 4, 256, 64)   8256        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 4, 256, 64)   0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 4, 256, 64)   0           dropout_3[0][0]                  \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 32, 32, 64)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 48)   3120        reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 96)   0           batch_normalization_14[0][0]     \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 64)   55360       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 128)  8192        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 32, 32, 128)  512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_10 (TFOpLambda)      (None, 32, 32, 128)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 33, 33, 128)  0           tf.nn.silu_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "depthwise_conv2d_5 (DepthwiseCo (None, 16, 16, 128)  1152        zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         depthwise_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_11 (TFOpLambda)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 64)   8192        tf.nn.silu_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 80)   46160       batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 80)   6480        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 4, 64, 80)    0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_4 (LayerNor (None, 4, 64, 80)    160         reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_2 (MultiHe (None, 4, 64, 80)    51760       layer_normalization_4[0][0]      \n",
            "                                                                 layer_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 64, 80)    0           multi_head_attention_2[0][0]     \n",
            "                                                                 reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_5 (LayerNor (None, 4, 64, 80)    160         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 4, 64, 160)   12960       layer_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 4, 64, 160)   0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 4, 64, 80)    12880       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 4, 64, 80)    0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 4, 64, 80)    0           dropout_5[0][0]                  \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_6 (LayerNor (None, 4, 64, 80)    160         add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_3 (MultiHe (None, 4, 64, 80)    51760       layer_normalization_6[0][0]      \n",
            "                                                                 layer_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 4, 64, 80)    0           multi_head_attention_3[0][0]     \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_7 (LayerNor (None, 4, 64, 80)    160         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 4, 64, 160)   12960       layer_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 4, 64, 160)   0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 4, 64, 80)    12880       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 4, 64, 80)    0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 4, 64, 80)    0           dropout_7[0][0]                  \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_8 (LayerNor (None, 4, 64, 80)    160         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_4 (MultiHe (None, 4, 64, 80)    51760       layer_normalization_8[0][0]      \n",
            "                                                                 layer_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 4, 64, 80)    0           multi_head_attention_4[0][0]     \n",
            "                                                                 add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_9 (LayerNor (None, 4, 64, 80)    160         add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 4, 64, 160)   12960       layer_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 4, 64, 160)   0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 4, 64, 80)    12880       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 4, 64, 80)    0           dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 4, 64, 80)    0           dropout_9[0][0]                  \n",
            "                                                                 add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_10 (LayerNo (None, 4, 64, 80)    160         add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_5 (MultiHe (None, 4, 64, 80)    51760       layer_normalization_10[0][0]     \n",
            "                                                                 layer_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 4, 64, 80)    0           multi_head_attention_5[0][0]     \n",
            "                                                                 add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_11 (LayerNo (None, 4, 64, 80)    160         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 4, 64, 160)   12960       layer_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 4, 64, 160)   0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 4, 64, 80)    12880       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 4, 64, 80)    0           dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 4, 64, 80)    0           dropout_11[0][0]                 \n",
            "                                                                 add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 16, 16, 80)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 64)   5184        reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 16, 16, 128)  0           batch_normalization_17[0][0]     \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 80)   92240       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 160)  12800       conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 160)  640         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_12 (TFOpLambda)      (None, 16, 16, 160)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPadding2D (None, 17, 17, 160)  0           tf.nn.silu_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "depthwise_conv2d_6 (DepthwiseCo (None, 8, 8, 160)    1440        zero_padding2d_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 160)    640         depthwise_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.silu_13 (TFOpLambda)      (None, 8, 8, 160)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 80)     12800       tf.nn.silu_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 80)     320         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 96)     69216       batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 96)     9312        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 4, 16, 96)    0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_12 (LayerNo (None, 4, 16, 96)    192         reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_6 (MultiHe (None, 4, 16, 96)    74400       layer_normalization_12[0][0]     \n",
            "                                                                 layer_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 4, 16, 96)    0           multi_head_attention_6[0][0]     \n",
            "                                                                 reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_13 (LayerNo (None, 4, 16, 96)    192         add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 4, 16, 192)   18624       layer_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 4, 16, 192)   0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 4, 16, 96)    18528       dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 4, 16, 96)    0           dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 4, 16, 96)    0           dropout_13[0][0]                 \n",
            "                                                                 add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_14 (LayerNo (None, 4, 16, 96)    192         add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_7 (MultiHe (None, 4, 16, 96)    74400       layer_normalization_14[0][0]     \n",
            "                                                                 layer_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 4, 16, 96)    0           multi_head_attention_7[0][0]     \n",
            "                                                                 add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_15 (LayerNo (None, 4, 16, 96)    192         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 4, 16, 192)   18624       layer_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 4, 16, 192)   0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 4, 16, 96)    18528       dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 4, 16, 96)    0           dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 4, 16, 96)    0           dropout_15[0][0]                 \n",
            "                                                                 add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_16 (LayerNo (None, 4, 16, 96)    192         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_8 (MultiHe (None, 4, 16, 96)    74400       layer_normalization_16[0][0]     \n",
            "                                                                 layer_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 4, 16, 96)    0           multi_head_attention_8[0][0]     \n",
            "                                                                 add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_17 (LayerNo (None, 4, 16, 96)    192         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 4, 16, 192)   18624       layer_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 4, 16, 192)   0           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 4, 16, 96)    18528       dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 4, 16, 96)    0           dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 4, 16, 96)    0           dropout_17[0][0]                 \n",
            "                                                                 add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 8, 8, 96)     0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 80)     7760        reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 8, 8, 160)    0           batch_normalization_20[0][0]     \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 96)     138336      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 320)    31040       conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 320)          0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 5)            1605        global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 1,307,621\n",
            "Trainable params: 1,305,077\n",
            "Non-trainable params: 2,544\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw7tkcXuL017"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We will be using the\n",
        "[`tf_flowers`](https://www.tensorflow.org/datasets/catalog/tf_flowers)\n",
        "dataset to demonstrate the model. Unlike other Transformer-based architectures,\n",
        "MobileViT uses a simple augmentation pipeline primarily because it has the properties\n",
        "of a CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G_i6ErEL017"
      },
      "source": [
        "batch_size = 64\n",
        "auto = tf.data.AUTOTUNE\n",
        "resize_bigger = 280\n",
        "num_classes = 5\n",
        "\n",
        "\n",
        "def preprocess_dataset(is_training=True):\n",
        "    def _pp(image, label):\n",
        "        if is_training:\n",
        "            # Resize to a bigger spatial resolution and take the random\n",
        "            # crops.\n",
        "            image = tf.image.resize(image, (resize_bigger, resize_bigger))\n",
        "            image = tf.image.random_crop(image, (image_size, image_size, 3))\n",
        "            image = tf.image.random_flip_left_right(image)\n",
        "        else:\n",
        "            image = tf.image.resize(image, (image_size, image_size))\n",
        "        label = tf.one_hot(label, depth=num_classes)\n",
        "        return image, label\n",
        "\n",
        "    return _pp\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset, is_training=True):\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(batch_size * 10)\n",
        "    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=auto)\n",
        "    return dataset.batch(batch_size).prefetch(auto)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLopVeJsL017"
      },
      "source": [
        "The authors use a multi-scale data sampler to help the model learn representations of\n",
        "varied scales. In this example, we discard this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtnKAq4TL017"
      },
      "source": [
        "## Load and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo-NkURML018",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bd7786-8051-452a-e56b-e3657ff6d4ce"
      },
      "source": [
        "train_dataset, val_dataset = tfds.load(\n",
        "    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True\n",
        ")\n",
        "\n",
        "num_train = train_dataset.cardinality()\n",
        "num_val = val_dataset.cardinality()\n",
        "print(f\"Number of training examples: {num_train}\")\n",
        "print(f\"Number of validation examples: {num_val}\")\n",
        "\n",
        "train_dataset = prepare_dataset(train_dataset, is_training=True)\n",
        "val_dataset = prepare_dataset(val_dataset, is_training=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset tf_flowers/3.0.1 (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to /root/tensorflow_datasets/tf_flowers/3.0.1...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset tf_flowers is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
            "Number of training examples: 3303\n",
            "Number of validation examples: 367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrbzdaJfL018"
      },
      "source": [
        "## Train a MobileViT (XXS) model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Jvq2sZL018",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2233e4d-adbd-4185-848e-2c559705ff4c"
      },
      "source": [
        "learning_rate = 0.002\n",
        "label_smoothing_factor = 0.1\n",
        "epochs = 30\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing_factor)\n",
        "\n",
        "\n",
        "def run_experiment(epochs=epochs):\n",
        "    mobilevit_xxs = create_mobilevit(num_classes=num_classes)\n",
        "    mobilevit_xxs.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    mobilevit_xxs.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "    mobilevit_xxs.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = mobilevit_xxs.evaluate(val_dataset)\n",
        "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    return mobilevit_xxs\n",
        "\n",
        "\n",
        "mobilevit_xxs = run_experiment()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "52/52 [==============================] - 61s 645ms/step - loss: 1.3588 - accuracy: 0.4675 - val_loss: 1.6175 - val_accuracy: 0.1662\n",
            "Epoch 2/30\n",
            "52/52 [==============================] - 31s 596ms/step - loss: 1.1233 - accuracy: 0.6058 - val_loss: 1.9653 - val_accuracy: 0.1907\n",
            "Epoch 3/30\n",
            "52/52 [==============================] - 31s 596ms/step - loss: 1.0203 - accuracy: 0.6679 - val_loss: 1.7027 - val_accuracy: 0.1907\n",
            "Epoch 4/30\n",
            "52/52 [==============================] - 31s 595ms/step - loss: 0.9755 - accuracy: 0.7015 - val_loss: 2.0102 - val_accuracy: 0.1907\n",
            "Epoch 5/30\n",
            "52/52 [==============================] - 31s 598ms/step - loss: 0.9434 - accuracy: 0.7163 - val_loss: 2.4456 - val_accuracy: 0.1907\n",
            "Epoch 6/30\n",
            "52/52 [==============================] - 31s 604ms/step - loss: 0.9401 - accuracy: 0.7227 - val_loss: 2.4911 - val_accuracy: 0.1907\n",
            "Epoch 7/30\n",
            "52/52 [==============================] - 31s 594ms/step - loss: 0.9026 - accuracy: 0.7448 - val_loss: 3.1809 - val_accuracy: 0.1907\n",
            "Epoch 8/30\n",
            "52/52 [==============================] - 31s 595ms/step - loss: 0.8960 - accuracy: 0.7463 - val_loss: 2.5937 - val_accuracy: 0.2262\n",
            "Epoch 9/30\n",
            "52/52 [==============================] - 31s 590ms/step - loss: 0.8683 - accuracy: 0.7678 - val_loss: 3.1227 - val_accuracy: 0.2561\n",
            "Epoch 10/30\n",
            "52/52 [==============================] - 31s 593ms/step - loss: 0.8315 - accuracy: 0.7781 - val_loss: 1.2631 - val_accuracy: 0.6076\n",
            "Epoch 11/30\n",
            "52/52 [==============================] - 31s 595ms/step - loss: 0.8177 - accuracy: 0.7853 - val_loss: 1.5448 - val_accuracy: 0.5150\n",
            "Epoch 12/30\n",
            "52/52 [==============================] - 31s 596ms/step - loss: 0.8113 - accuracy: 0.7984 - val_loss: 1.0199 - val_accuracy: 0.6894\n",
            "Epoch 13/30\n",
            "52/52 [==============================] - 31s 594ms/step - loss: 0.7929 - accuracy: 0.8129 - val_loss: 1.0856 - val_accuracy: 0.7030\n",
            "Epoch 14/30\n",
            "52/52 [==============================] - 31s 603ms/step - loss: 0.7635 - accuracy: 0.8144 - val_loss: 0.9572 - val_accuracy: 0.7003\n",
            "Epoch 15/30\n",
            "52/52 [==============================] - 31s 591ms/step - loss: 0.7673 - accuracy: 0.8196 - val_loss: 0.9980 - val_accuracy: 0.7411\n",
            "Epoch 16/30\n",
            "52/52 [==============================] - 31s 596ms/step - loss: 0.7467 - accuracy: 0.8308 - val_loss: 1.2711 - val_accuracy: 0.5995\n",
            "Epoch 17/30\n",
            "52/52 [==============================] - 31s 592ms/step - loss: 0.7577 - accuracy: 0.8235 - val_loss: 0.8860 - val_accuracy: 0.7493\n",
            "Epoch 18/30\n",
            "52/52 [==============================] - 31s 599ms/step - loss: 0.7212 - accuracy: 0.8380 - val_loss: 1.0692 - val_accuracy: 0.7139\n",
            "Epoch 19/30\n",
            "52/52 [==============================] - 31s 594ms/step - loss: 0.7071 - accuracy: 0.8492 - val_loss: 1.1552 - val_accuracy: 0.6894\n",
            "Epoch 20/30\n",
            "52/52 [==============================] - 31s 588ms/step - loss: 0.7186 - accuracy: 0.8453 - val_loss: 0.8242 - val_accuracy: 0.8038\n",
            "Epoch 21/30\n",
            "52/52 [==============================] - 31s 593ms/step - loss: 0.6987 - accuracy: 0.8523 - val_loss: 0.9391 - val_accuracy: 0.7221\n",
            "Epoch 22/30\n",
            "52/52 [==============================] - 31s 598ms/step - loss: 0.7068 - accuracy: 0.8498 - val_loss: 1.0295 - val_accuracy: 0.7084\n",
            "Epoch 23/30\n",
            "52/52 [==============================] - 31s 600ms/step - loss: 0.6958 - accuracy: 0.8538 - val_loss: 0.8844 - val_accuracy: 0.7847\n",
            "Epoch 24/30\n",
            "52/52 [==============================] - 31s 594ms/step - loss: 0.6894 - accuracy: 0.8559 - val_loss: 0.9847 - val_accuracy: 0.7193\n",
            "Epoch 25/30\n",
            "52/52 [==============================] - 31s 594ms/step - loss: 0.6786 - accuracy: 0.8668 - val_loss: 1.0279 - val_accuracy: 0.6921\n",
            "Epoch 26/30\n",
            "52/52 [==============================] - 31s 596ms/step - loss: 0.6585 - accuracy: 0.8771 - val_loss: 0.8753 - val_accuracy: 0.8038\n",
            "Epoch 27/30\n",
            "52/52 [==============================] - 31s 599ms/step - loss: 0.6472 - accuracy: 0.8804 - val_loss: 0.9183 - val_accuracy: 0.7575\n",
            "Epoch 28/30\n",
            "52/52 [==============================] - 31s 597ms/step - loss: 0.6447 - accuracy: 0.8828 - val_loss: 0.9384 - val_accuracy: 0.7384\n",
            "Epoch 29/30\n",
            "52/52 [==============================] - 31s 596ms/step - loss: 0.6511 - accuracy: 0.8747 - val_loss: 0.8189 - val_accuracy: 0.7956\n",
            "Epoch 30/30\n",
            "52/52 [==============================] - 31s 601ms/step - loss: 0.6414 - accuracy: 0.8753 - val_loss: 0.7301 - val_accuracy: 0.8365\n",
            "6/6 [==============================] - 1s 122ms/step - loss: 0.7301 - accuracy: 0.8365\n",
            "Validation accuracy: 83.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BX8XqVrL019"
      },
      "source": [
        "## Results and TFLite conversion\n",
        "\n",
        "With about one million parameters, getting to ~85% top-1 accuracy on 256x256 resolution is\n",
        "a strong result. This MobileViT mobile is fully compatible with TensorFlow Lite (TFLite)\n",
        "and can be converted with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUksn4E9L019"
      },
      "source": [
        "# Serialize the model as a SavedModel.\n",
        "mobilevit_xxs.save(\"mobilevit_xxs\")\n",
        "\n",
        "# Convert to TFLite. This form of quantization is called\n",
        "# post-training dynamic-range quantization in TFLite.\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"mobilevit_xxs\")\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS,  # Enable TensorFlow ops.\n",
        "]\n",
        "tflite_model = converter.convert()\n",
        "open(\"mobilevit_xxs.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shLnjsszL019"
      },
      "source": [
        "To learn more about different quantization recipes available in TFLite and running\n",
        "inference with TFLite models, check out\n",
        "[this official resource](https://www.tensorflow.org/lite/performance/post_training_quantization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J_LCLLiJkj0"
      },
      "source": [
        "# Share on Hugging Face Hub 🤗"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVnHd8XkJolh"
      },
      "source": [
        "%%capture\n",
        "! pip install huggingface_hub\n",
        "! apt install git-lfs\n",
        "! git config --global credential.helper store"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oIsiABRJzkX",
        "outputId": "c2978a27-67ee-452c-f9e3-ecbbf2c1d9b6"
      },
      "source": [
        "! huggingface-cli login"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        \n",
            "Username: nateraw\n",
            "Password: \n",
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2mcpjL2Nb79"
      },
      "source": [
        "from huggingface_hub import push_to_hub_keras"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445,
          "referenced_widgets": [
            "2fa20d982beb464d9e776ac5faf58002",
            "78fa9b127bcf4f07a92e5f0724dd9b7c",
            "2dc5fb520f064b8e8ab75e293af6cf15",
            "af34bc8d60cc4964a366eca342e890fc",
            "094b25b750934cbda7318a42aa28d378",
            "9f2a06b8c7fc42239ea2e9e813334959",
            "0c39ae1be7d343ac87e582f86cad0426",
            "58b5302d0d394793adb1777883caf19a",
            "ad45153867e84d0ca95b068795e2de50",
            "8a46cf77f32d4271b6e1aad127f325f4",
            "45f2cdd52d7547bc9b386c1ef1c6e130",
            "e986d9c150884880b5642ac4d4a720c5",
            "b8f9791b270947ae81b06593c5488333",
            "143f919ec7a1441989b3bde75d7eb503",
            "67a835314e864046972f4831a3aadf09",
            "dfcef6d7579842d0a4615cd0658fab77",
            "83171e6892c8484daeb1d273317223fe",
            "328c94a6e9ef47a0abce08a5ab8b9445",
            "959b4d991d0f4e6d848ac67afd6839f3",
            "d5a97a2c046e4449b99cdfb226d209d8",
            "fd5a8fceb9424d4a867294478a49de3d",
            "64f07caa84e3478c85c6c8c12f371e0d",
            "d36f6585a50745818991e9bb21fc9dab",
            "3abeebd840f64aa88b24790cad9e2c99",
            "9f5165422c8f46528987200122fc34b0",
            "0c6ce09eaf344a22b89d369ecdc454fa",
            "61718b42d97e4b7fbf928bc0b3cb548a",
            "0ca3478aea394bc6b96f0632ff52d82a",
            "a66f54459f4d4006ab5db0625ebca351",
            "426bab5dd71d453f8e1a34ffc1680ec7",
            "df8121f43bff42858dfc0f79769f515c",
            "3650b1615abb4864a265455723e16d4b",
            "eb9d04dfcd784f0d8c0c7904d4dd8d3e"
          ]
        },
        "id": "E4KzbjyYM2Wy",
        "outputId": "47c9b008-90c4-4f74-a90c-4ec844c8a693"
      },
      "source": [
        "classes = [\"dandelion\", \"daisy\", \"tulips\", \"sunflowers\", \"roses\"]\n",
        "id2label = {str(i): c for i, c in enumerate(classes)}\n",
        "\n",
        "push_to_hub_keras(mobilevit_xxs, 'keras-mobilevit-xxs-flowers', config={'id2label': id2label})"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py:718: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
            "  FutureWarning,\n",
            "Cloning https://huggingface.co/nateraw/keras-mobilevit-xxs-flowers into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/nateraw/keras-mobilevit-xxs-flowers into local empty directory.\n",
            "WARNING:absl:Found untraced functions such as query_layer_call_and_return_conditional_losses, query_layer_call_fn, key_layer_call_and_return_conditional_losses, key_layer_call_fn, value_layer_call_and_return_conditional_losses while saving (showing 5 of 270). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: keras-mobilevit-xxs-flowers/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: keras-mobilevit-xxs-flowers/assets\n",
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "Adding files tracked by Git LFS: ['variables/variables.data-00000-of-00001']. This may take a bit of time if the files are large.\n",
            "WARNING:huggingface_hub.repository:Adding files tracked by Git LFS: ['variables/variables.data-00000-of-00001']. This may take a bit of time if the files are large.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fa20d982beb464d9e776ac5faf58002",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Upload file variables/variables.data-00000-of-00001:   0%|          | 3.38k/15.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e986d9c150884880b5642ac4d4a720c5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Upload file keras_metadata.pb:   1%|          | 3.38k/358k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d36f6585a50745818991e9bb21fc9dab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Upload file saved_model.pb:   0%|          | 3.38k/4.77M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "To https://huggingface.co/nateraw/keras-mobilevit-xxs-flowers\n",
            "   ed0cf2f..f73f492  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:To https://huggingface.co/nateraw/keras-mobilevit-xxs-flowers\n",
            "   ed0cf2f..f73f492  main -> main\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://huggingface.co/nateraw/keras-mobilevit-xxs-flowers/commit/f73f4923178e26fe79ef0b8dfa0a0cfdb564b21a'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}